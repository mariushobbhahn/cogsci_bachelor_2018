% Main chapter 1:
% Introduction 

\chapter{Introduction}
\label{chap:Introduction}

Learning a forward model and then using temporal gradients has been shown to generate active-inference-based, flexible, adaptive goal-directed behavior in the domain of planning future trajectories in partially observable environments \cite{RocketballOtte2017}. The model has been successfully extended to not only include the prospective inference but also a retrospective inference process, i.e. inferring the unobservable contextual state that best explains its recently encountered sensorimotor experiences \cite{REPRISE2018}. In both cases, the gradient based inference was not only highly successful but even able to be used in online applications for very fast moving vehicles in partially observable environments. \\
Autoencoders are a class of artificial neural networks, combining an encoder and a decoder network. The encoder is used to reduce the dimensionality of the input data to a minimum while still maintaining all necessary features. The space of all possible options after the dimensionality reduction is called latent space. The decoder reconstructs the original input from the reduced dimensionality of the latent space. The autoencoder therefore extracts the essential features from the input to then generate the inputs again from these essential features. Many different versions of autoencoders have been published since their first appearance, including but not limited to: Variational Autoencoders \cite{2013VAE} and Denoising Autoencoders \cite{DenoisingAutoencodersVincent2010}. \\
Generative models have been able to create sequences that are indistinguishable to sequences created by humans. In the field of handwritten characters, Nikolaus (\cite{2018Nikolaus}) was able to show that a recurrent neural network (RNN) with the usage of long short-term memory (LSTM) is able to generate characters and variations of characters, by combining different versions of learned data. \\
This work is extracting and combining ideas from the previously described concepts. It attempts to implement a generative model to create handwritten characters and use the temporal gradients to then infer the original inputs when given certain outputs. Essentially it is like an autoencoder in the same model, where the main processes swapped positions. The forward process is the decoder part while the inverse classification can be thought of an encoder. \\
If the technique can be successfully deployed multiple advantages can be gained. First, there would be one model that can generate and classify at the same time. This means that only half the training time is necessary in comparison to training a generator and a classifier individually. Second, significantly less training data could be used. If the model is able to generate the sequences, it very likely learned a concept about the essential features of that sequence and its differentiation to other concepts. This includes different variations of this sequence and therefore makes massive amounts of training data unnecessary. This would speed up the training process and make classification problems, for which only few data exist, possible to be classified. 

 